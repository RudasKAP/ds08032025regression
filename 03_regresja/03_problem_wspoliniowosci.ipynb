{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"9AQrOdajQbU_"},"outputs":[],"source":["# import numpy as np\n","# import pandas as pd\n","# from sklearn.linear_model import LinearRegression\n","# from statsmodels.stats.outliers_influence import variance_inflation_factor\n","# from statsmodels.tools.tools import add_constant\n","# import statsmodels.api as sm\n","# from scipy import stats\n","# from patsy import dmatrices\n","\n","# import matplotlib.pyplot as plt"]},{"cell_type":"markdown","metadata":{"id":"Rh2so0xcQbVB"},"source":["# Problem współliniowości"]},{"cell_type":"markdown","metadata":{"id":"AsR3vrUUQbVD"},"source":["### Diagnostyka\n","\n","1. **Macierz korelacji predyktorów** - $D_X = (\\rho_{ij})$;\n","\n","2. **Uwarunkowanie macierzy** $\\frac{\\lambda_{\\text{max}}(D_X)}{\\lambda_{\\text{min}}(D_X)}$ - duże $\\implies$ istnieje para predyktorów zależnych liniowo;\n","\n","3. **VIF** (ang. *variance inflation factor*) - współczynnik podbicia wariancji\n","\n","    Dla $1\\leq i \\leq p-1$: $$R^2_i = \\frac{\\text{RSS}}{\\text{TSS}}$$ dla modelu $x_i \\sim x_{-i}$, gdzie $x_{-i}$ oznacza wszystkie zmienne objaśniające z  pominięciem $i$-tej.\n","\n","    Wówczas\n","    $$\n","    \\text{VIF}_i = \\frac{1}{1-R_i^2}\n","    $$\n","\n","    **Interpretacja:** Duża wartość dla pewnego $i$ wskazuje na potencjalną liniową zależność $i$-tej zmiennej objaśniającej od pozostałych zmiennych.\n","\n","    **Reguła kciuka:** Jeśli $\\text{VIF}_i\\geq 10$, to $i$-tą zmienną uznajemy w przybliżeniu liniowo zależną od pozostałych."]},{"cell_type":"markdown","metadata":{"id":"ldihMp89QbVE"},"source":["# Zadanie 1\n","Dla danych `Carseats` sprawdź, czy występuje w nich problem współliniowości przy użyciu powyższych metod. Jeśli tak, odrzuć ze zbioru zmienne zależne liniowo i dopasuj model regresji liniowej bez nich. Porównaj wyniki."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DYuM4Xz0QbVE"},"outputs":[],"source":["# carseats = sm.datasets.get_rdataset(dataname=\"Carseats\", package=\"ISLR\", cache=True)\n","# carseats_df = carseats.data\n","# carseats_df = pd.get_dummies(carseats_df,columns = ['ShelveLoc','Urban','US'],drop_first=True)"]},{"cell_type":"code","source":["# #X ma być bez Sales\n","# Xvar = carseats_df.loc[:, ~carseats_df.columns.isin(['Sales'])]\n","# Xvar"],"metadata":{"id":"Z8vLTY_VvidT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# features = Xvar.columns[np.arange(0,7)] #tylko ciągłe\n","# correlation_matrix = Xvar[features].corr()\n","# print(correlation_matrix)"],"metadata":{"id":"9hyRuM3lvlRm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# #zrobimy heatmapę od wartości bezwzględnych 0 - brak koralcji,  1-mocna korelacja\n","# import matplotlib as mpl\n","# heatmap = plt.pcolor(np.abs(correlation_matrix), cmap=mpl.cm.coolwarm, alpha=0.8)\n","\n","# heatmap.axes.set_frame_on(False)\n","# heatmap.axes.set_yticks(np.arange(correlation_matrix.shape[0]) + 0.5, minor=False)\n","# heatmap.axes.set_xticks(np.arange(correlation_matrix.shape[1]) + 0.5, minor=False)\n","# heatmap.axes.set_xticklabels(features, minor=False)\n","# plt.xticks(rotation=90)\n","# heatmap.axes.set_yticklabels(features, minor=False)\n","# plt.tick_params(axis='both', which='both', bottom='off',\n","#                     top='off', left='off', right='off')\n","# plt.colorbar()\n","# plt.show()"],"metadata":{"id":"2aqfD73Ivncq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# y = carseats.data['Sales']\n","# lm = LinearRegression()\n","# lm.fit(Xvar.values,y)\n","# lm.coef_"],"metadata":{"id":"pv9bV5JXvq5-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Xvif = add_constant(np.array(Xvar.values,dtype=float))\n","# vif = pd.DataFrame()\n","# vif[\"VIF Factor\"] = [variance_inflation_factor(Xvif, i) for i in range(1,Xvif.shape[1])]\n","# vif[\"features\"] = Xvar.columns\n","# vif"],"metadata":{"id":"t6lzxsZ2vv6K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#jest ok, nie trzeba nic wyrzucać"],"metadata":{"id":"-Sd4EOu0jywh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"w4lwRB6vQbVI"},"source":["# Zadanie 2\n","Wczytaj dane `kc_house_data.csv` ([This dataset contains house sale prices for King County, which includes Seattle. It includes homes sold between May 2014 and May 2015](https://www.kaggle.com/harlfoxem/housesalesprediction/data)).\n","\n","Dopasuj model `price ~ bathrooms + sqft_living + sqft_lot + sqft_above + sqft_basement + lat + long`, uwzględnij współliniowość predyktorów."]},{"cell_type":"code","source":["# house = pd.read_csv('kc_house_data.csv')\n","# house.head()"],"metadata":{"id":"lJc7BrXhv0QB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# columns = np.array(['bathrooms', 'sqft_living', 'sqft_lot',\n","#                     'sqft_above', 'sqft_basement', 'lat', 'long'])\n","# Xvar = house.loc[:, house.columns.isin(columns)]\n","# y = house['price']"],"metadata":{"id":"H-p1IDQ8lZck"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# #heatmapa od wartości bezwzględnych korelacji - im bliżej 1 tym większe skorelowanie\n","# correlation_matrix = house[columns].corr()\n","# heatmap = plt.pcolor(np.abs(correlation_matrix),alpha =0.8,cmap=mpl.cm.coolwarm)\n","# heatmap.axes.set_frame_on(False)\n","# heatmap.axes.set_yticks(np.arange(correlation_matrix.shape[0]) + 0.5, minor=False)\n","# heatmap.axes.set_xticks(np.arange(correlation_matrix.shape[1]) + 0.5, minor=False)\n","# heatmap.axes.set_xticklabels(columns, minor=False)\n","# plt.xticks(rotation=90)\n","# heatmap.axes.set_yticklabels(columns, minor=False)\n","# plt.tick_params(axis='both', which='both', bottom='off',\n","#                     top='off', left='off', right='off')\n","# plt.colorbar()\n","# plt.show()\n"],"metadata":{"id":"r86Iml9xv7Kx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"4MYO08Kbv_g5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"1mPCYHyZv_YF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"SPiOgZhHv_Od"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jh1bhUQJQbVK"},"source":["# Zadanie 3\n","Wczytaj zbiór `Hald.csv`. Znajdź najlepszy model regresji liniowej uwzględniając współliniowość predyktorów.\n","\n","Opis zbioru:\n","\n","    Heat evolved during setting of 13 cement mixtures of four basic ingredients. Each ingredient percentage appears to be rounded down to a full integer. The sum of the four mixture percentages varies from a maximum of 99% to a minimum of 95%. If all four regressor X-variables always summed to 100%, the centered X-matrix would then be of rank only 3. Thus, the regression of heat on four X-percentages is ill-conditioned, with an approximate rank deficiency of MCAL = 1. The first column is the response and the remaining four columns are the predictors."]},{"cell_type":"code","source":["# hald = pd.read_csv(\"Hald.csv\")\n","# hald.head()"],"metadata":{"id":"wwKJau0KwJUh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Xvar = hald.loc[:, hald.columns!='y']\n","# y = hald['y']"],"metadata":{"id":"lHY7_i0DosnJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"79lQ4y1fwEXV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"409gPtviwEOZ"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}